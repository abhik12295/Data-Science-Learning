{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NLP: Natural Language Processing\n",
    "Definition: \n",
    "A subfield of artificial intelligence that focuses on the interaction between \n",
    "computers and humans through natural language.\n",
    "\n",
    "Applications:\n",
    "1. Sentiment Analysis\n",
    "2. Machine Translation\n",
    "3. Chatbots\n",
    "4. Information Retrieval\n",
    "5. Text Summarization\n",
    "\n",
    "Basics:\n",
    "1. Tokenization\n",
    "2. Stopword Removal\n",
    "3. Stemming and Lemmatization\n",
    "4. Part-of-Speech Tagging\n",
    "5. Named Entity Recognition\n",
    "\n",
    "How it works:\n",
    "NLP works by using algorithms and models to process and analyze large amounts of natural language data. \n",
    "This involves several steps, including:\n",
    "\n",
    "1. Text Preprocessing: Cleaning and preparing the text data for analysis, which may include tokenization, \n",
    "stopword removal, and stemming.\n",
    "\n",
    "2. Feature Extraction: Converting the text data into a numerical format that can be used by \n",
    "machine learning algorithms. This may involve techniques such as bag-of-words, TF-IDF, or word embeddings.\n",
    "\n",
    "3. Model Training: Using labeled data to train machine learning models to perform specific NLP tasks, \n",
    "such as sentiment analysis or named entity recognition.\n",
    "\n",
    "4. Inference: Applying the trained models to new, unseen text data to make predictions or extract insights.\n",
    "\n",
    "5. Post-processing: Refining the model outputs and integrating them into applications or workflows.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Roadmap for NLP:\n",
    "1. Learn the basics of NLP and its applications.\n",
    "2. Familiarize yourself with text preprocessing techniques.\n",
    "    a. Tokenization\n",
    "    b. Stopword Removal\n",
    "    c. Stemming and Lemmatization\n",
    "    d. Part-of-Speech Tagging\n",
    "    e. Named Entity Recognition\n",
    "3. Explore feature extraction methods. (Text Representation - Input texts to vectors)\n",
    "    a. Bag-of-Words\n",
    "    b. TF-IDF\n",
    "    c. Uni-grams\n",
    "    d. Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "4. Understand different NLP models and algorithms.\n",
    "    a. Rule-based Models\n",
    "    b. Machine Learning Models (e.g., SVM, Random Forest)\n",
    "    c. Deep Learning Models (e.g., GRU, RNN, LSTM, Transformers)\n",
    "5. Word Embeddings\n",
    "    a. Word2Vec\n",
    "    b. GloVe\n",
    "    c. FastText\n",
    "6. Transformers\n",
    "    a. BERT\n",
    "    b. GPT\n",
    "    c. T5\n",
    "7. Hugging Face Transformers - A library for state-of-the-art NLP models.\n",
    "8. BERT\n",
    "   a. Architecture\n",
    "   b. Pre-training\n",
    "   c. Fine-tuning\n",
    "9. Work on real-world NLP projects to gain hands-on experience.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734909af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Practical Applications of NLP:\n",
    "1. Sentiment Analysis: Determining the sentiment behind a piece of text (e.g., positive, negative, neutral).\n",
    "2. Chatbots: Building conversational agents that can understand and respond to user queries.\n",
    "3. Machine Translation: Automatically translating text from one language to another.\n",
    "4. Text Summarization: Generating concise summaries of long documents.\n",
    "5. Named Entity Recognition: Identifying and classifying key entities in text (e.g., names, dates, locations).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c8f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenization in NLP:\n",
    "\n",
    "Definition:\n",
    "Tokenization is the process of breaking down a text into smaller units, called tokens. \n",
    "These tokens can be words, phrases, or even individual characters, \n",
    "depending on the level of granularity required for the NLP task at hand. \n",
    "Tokenization is a crucial step in NLP as it helps in understanding the structure and meaning of the text.\n",
    "\n",
    "\n",
    "Topics:\n",
    "a. Corpus: A corpus is a large and structured set of texts (or speech) that is used for \n",
    "linguistic analysis and model training in NLP. It serves as the foundational dataset for various NLP tasks, \n",
    "providing the necessary context and examples for algorithms to learn from.\n",
    "b. Token: A token is an individual unit of text that has been extracted from a larger body of text \n",
    "during the tokenization process. \n",
    "Tokens can be words, phrases, or even characters, depending on the level of granularity required for the NLP task.  \n",
    "c. Documents: Sentences or larger bodies of text that are processed as a whole during NLP tasks.\n",
    "d. Vocabulary: The set of all unique tokens (words, phrases, etc.) present in a corpus or dataset. \n",
    "            A rich vocabulary is essential for effective NLP applications.\n",
    "e. Words: The individual tokens that make up a text, which can be analyzed and processed for various NLP tasks.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tokenization\n",
    "2 important library:\n",
    "a. nltk - Natural Language Toolkit, a powerful library for text processing and analysis.\n",
    "b. spacy - An industrial-strength NLP library that provides fast and efficient tools for various NLP tasks.\n",
    "\n",
    "Difference between nltk and spacy:\n",
    "\n",
    "1. Ease of Use:\n",
    "   - NLTK: More flexible and allows for fine-grained control over text processing tasks, \n",
    "            but has a steeper learning curve.\n",
    "   - SpaCy: Designed for production use, with a simpler API and better performance out of the box.\n",
    "\n",
    "2. Speed:\n",
    "   - NLTK: Generally slower, as it is more focused on providing a wide range of tools and resources.\n",
    "   - SpaCy: Optimized for speed and efficiency, making it suitable for large-scale applications.\n",
    "\n",
    "3. Pre-trained Models:\n",
    "   - NLTK: Offers a variety of pre-trained models, but they may not be as advanced as those in SpaCy.\n",
    "   - SpaCy: Provides state-of-the-art pre-trained models for various languages and tasks, \n",
    "            making it easier to get started.\n",
    "\n",
    "4. Features:\n",
    "   - NLTK: A comprehensive library with a wide range of tools for text processing, including tokenization, \n",
    "            stemming, and parsing.\n",
    "   - SpaCy: Focuses on providing a streamlined set of features for common NLP tasks, such as named \n",
    "            entity recognition and dependency parsing.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54ecb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0577662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello, from the other world, My name is Abhishek.\n",
    "Please do learn the entire course to become expert in NLP! Come here for more to learn about NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74731d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, from the other world, My name is Abhishek.\\nPlease do learn the entire course to become expert in NLP! Come here for more to learn about NLP.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb016ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# sentences -> paragraphs\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt_tab')\n",
    "sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b39e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, from the other world, My name is Abhishek.',\n",
       " 'Please do learn the entire course to become expert in NLP!',\n",
       " 'Come here for more to learn about NLP.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e7e4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, from the other world, My name is Abhishek.\n",
      "Please do learn the entire course to become expert in NLP!\n",
      "Come here for more to learn about NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03610cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'from',\n",
       " 'the',\n",
       " 'other',\n",
       " 'world',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Abhishek',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'course',\n",
       " 'to',\n",
       " 'become',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '!',\n",
       " 'Come',\n",
       " 'here',\n",
       " 'for',\n",
       " 'more',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = word_tokenize(corpus)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cc60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello',\n",
       "  ',',\n",
       "  'from',\n",
       "  'the',\n",
       "  'other',\n",
       "  'world',\n",
       "  ',',\n",
       "  'My',\n",
       "  'name',\n",
       "  'is',\n",
       "  'Abhishek',\n",
       "  '.'],\n",
       " ['Please',\n",
       "  'do',\n",
       "  'learn',\n",
       "  'the',\n",
       "  'entire',\n",
       "  'course',\n",
       "  'to',\n",
       "  'become',\n",
       "  'expert',\n",
       "  'in',\n",
       "  'NLP',\n",
       "  '!'],\n",
       " ['Come', 'here', 'for', 'more', 'to', 'learn', 'about', 'NLP', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization - paragraph to word or sentence to word\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb5e8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = '''Hello Abhishek. NLP's are better than ML for Text analysis.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "618ca66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Abhishek',\n",
       " '.',\n",
       " 'NLP',\n",
       " \"'\",\n",
       " 's',\n",
       " 'are',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ML',\n",
       " 'for',\n",
       " 'Text',\n",
       " 'analysis',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "# breaks 's as well - punctuation aware\n",
    "word_list = wordpunct_tokenize(corpus1)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd994f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Abhishek.',\n",
       " 'NLP',\n",
       " \"'s\",\n",
       " 'are',\n",
       " 'better',\n",
       " 'than',\n",
       " 'ML',\n",
       " 'for',\n",
       " 'Text',\n",
       " 'analysis',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "word_list = tokenizer.tokenize(corpus1)\n",
    "word_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
