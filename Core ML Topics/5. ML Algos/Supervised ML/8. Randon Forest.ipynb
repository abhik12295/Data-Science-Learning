{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensemble Techniques in Machine Learning:\n",
    "- Definition: Ensemble techniques combine multiple models to produce a better predictive performance \n",
    "than any individual model.\n",
    "- Types of Ensemble Methods:\n",
    "    1. Bagging (Bootstrap Aggregating):\n",
    "        - Combines predictions from multiple models trained on different subsets of the training data.\n",
    "        - Each model is trained independently, and their predictions are averaged (for regression) or voted on (for classification).\n",
    "        - Trains models in parallel, which can lead to faster training times.\n",
    "        - Base learners can be of the different types, but decision trees are commonly used.\n",
    "        (base learners are the individual models in the ensemble)\n",
    "        - Reduces variance and helps to avoid overfitting.\n",
    "        - Example: Random Forest, where multiple decision trees are trained on random subsets of the data.\n",
    "\n",
    "    2. Boosting:\n",
    "        - Sequentially trains models, where each new model focuses on correcting errors made by previous models.\n",
    "        - Models are trained in a way that each subsequent model pays more attention to the instances that were misclassified by previous models.\n",
    "        - Combines weak learners to create a strong learner.\n",
    "        - Can be more sensitive to noise in the data.\n",
    "        - Reduces bias and variance, often leading to better performance than bagging.\n",
    "        - Example: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "    3. Stacking:\n",
    "        - Combines multiple models (base learners) and uses another model (meta-learner) to make final predictions based on the outputs of the base learners.\n",
    "        - Can leverage the strengths of different algorithms.\n",
    "        - Example: Using logistic regression as a meta-learner for predictions from decision trees and SVMs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce762de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest Machine Learning Algorithm:\n",
    "\n",
    "- Definition: A Random Forest is an ensemble learning method (bagging) that constructs multiple decision trees \n",
    "            during training and outputs the mode of their predictions for classification \n",
    "            or the mean prediction for regression.\n",
    "- Key Features:\n",
    "    - Uses bagging to create a \"forest\" of decision trees.\n",
    "    - Combines the predictions of multiple decision trees to improve accuracy and control overfitting.\n",
    "    - Each tree is built from a random subset of the training data, and at each split, a random subset of features is considered.\n",
    "    - The final prediction is made by averaging the predictions of all trees (for regression) or taking a majority vote (for classification).\n",
    "    - Each tree is trained on a random subset of the data, which helps in reducing variance.\n",
    "    - Can handle both classification and regression tasks.\n",
    "\n",
    "- How it Works: (row sampling and feature selection)\n",
    "    - Randomly selects a subset of the training data (with replacement) to train each tree.\n",
    "    - At each node, it randomly selects a subset of features to consider for splitting, which helps in reducing correlation between trees.\n",
    "    - Each tree is grown to its maximum depth without pruning, which allows for capturing complex patterns in the data.\n",
    "    - The final prediction is made by aggregating the predictions from all trees.\n",
    "- Advantages:\n",
    "    - Robust to overfitting, especially with large datasets.\n",
    "    - Handles missing values and maintains accuracy for large datasets.\n",
    "    - Provides feature importance scores, which can be useful for feature selection.  \n",
    "- Disadvantages:\n",
    "    - Can be computationally intensive and slower to predict compared to a single decision tree.\n",
    "    - Less interpretable than a single decision tree due to the complexity of multiple trees. \n",
    "- Common Use Cases:\n",
    "    - Classification tasks such as spam detection, fraud detection, and image classification.\n",
    "    - Regression tasks like predicting house prices or stock prices.\n",
    "    - Feature selection and ranking in high-dimensional datasets.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random Forest Classification Implementation:\n",
    "'''\n",
    "# Holiday Package Prediction\n",
    "\n",
    "'''\n",
    "Problem statement:\n",
    "Trips and travel.com company wants to enable and establish a viable business model to expand\n",
    "the customer base. One of the ways to expand the customer base is to introdcue a new offering of package tours.\n",
    "Currently, there are 5 types of package tours available:\n",
    "1. Basic\n",
    "2. Standard\n",
    "3. Deluxe\n",
    "4. Super Deluxe\n",
    "5. king\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
