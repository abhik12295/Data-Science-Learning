{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30dbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decision Tree:\n",
    "\n",
    "- Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. \n",
    "- They work by splitting the data into subsets based on the value of input features, \n",
    "creating a tree-like structure where each node represents a feature and each branch represents a decision rule. \n",
    "- The process continues until a stopping criterion is met, \n",
    "such as reaching a maximum depth or having a minimum number of samples in a leaf node.\n",
    "- Decision trees are easy to interpret and visualize, making them a popular choice for many applications. \n",
    "- However they can be prone to overfitting, especially when the tree is deep and complex. \n",
    "To mitigate this, techniques such as pruning (removing branches that have little importance) or \n",
    "setting constraints on the tree's depth can be applied. \n",
    "\n",
    "\n",
    "Algorithms to construct decision trees:\n",
    "a. ID3:\n",
    "- ID3 (Iterative Dichotomiser 3) is an algorithm used to create a decision tree for classification tasks.\n",
    "- It uses a top-down, greedy approach to select the feature that provides the highest information gain at each node.\n",
    "- The algorithm continues to split the data based on the selected feature until all samples in a node belong to the same class or a stopping criterion is met.\n",
    "- ID3 is known for its simplicity and effectiveness, but it can lead to overfitting if not properly managed.\n",
    "\n",
    "b. CART:\n",
    "- CART (Classification and Regression Trees) is a decision tree algorithm that can be used for both classification and regression tasks.\n",
    "- It constructs binary trees by recursively splitting the data based on the feature that minimizes a cost function\n",
    "    (e.g., Gini impurity for classification or mean squared error for regression).\n",
    "- CART can handle both categorical and continuous features and is robust to overfitting through techniques like\n",
    "pruning.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decision Tree Classifier: \n",
    "- A decision tree classifier is a specific type of decision tree used for classification tasks.\n",
    "- It predicts the class label of an input sample by traversing the tree from the root to a leaf node,\n",
    "    where each internal node represents a feature and each branch represents a decision based on that feature.\n",
    "- The leaf nodes contain the predicted class labels.\n",
    "Eg: \n",
    "If a decision tree is trained on a dataset of animals, it might classify an animal as \"Mammal\" or \"Bird\" \n",
    "based on features like \"Has Fur\" or \"Can Fly\".\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Few things to remember:\n",
    "a. Purity:\n",
    "- Purity refers to the homogeneity of the samples in a node of a decision tree.\n",
    "- A node is considered pure if all samples belong to the same class.\n",
    "- High purity indicates that the decision tree has effectively separated the classes.\n",
    "- Common measures of purity include Gini impurity and entropy.\n",
    "    i. Gini impurity:\n",
    "        - Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled \n",
    "            if it was randomly labeled according to the distribution of labels in the subset.\n",
    "        - It ranges from 0 (perfectly pure) to 0.5 (maximum impurity for a binary classification).\n",
    "        - The formula for Gini impurity is:\n",
    "          Gini = 1 - Σ(p_i^2)\n",
    "          where p_i is the proportion of samples belonging to class i in the node.\n",
    "    ii. Entropy:\n",
    "        - Entropy is a measure of uncertainty or randomness in a set of samples.\n",
    "        - It is used to quantify the impurity of a node in a decision tree.\n",
    "        - The formula for entropy is:\n",
    "          Entropy = -Σ(p_i * log2(p_i))\n",
    "          where p_i is the proportion of samples belonging to class i in the node.\n",
    "\n",
    "b. Which feature to split on:\n",
    "- The feature to split on is chosen based on the one that maximizes the reduction in impurity (or increases purity) of the resulting nodes.\n",
    "- This is often done using metrics like information gain, Gini impurity, or entropy.\n",
    "    i. Information gain:\n",
    "        - Information gain measures the reduction in entropy after a split.\n",
    "        - It is calculated as the difference between the entropy of the parent node and the weighted average entropy of the child nodes.\n",
    "        - A higher information gain indicates a better feature for splitting.\n",
    "\n",
    "c. How to split:\n",
    "- The splitting process involves dividing the dataset into subsets based on the values of the selected feature.\n",
    "- For categorical features, the split can be based on the presence or absence of a category.\n",
    "- For continuous features, the split can be based on a threshold value (e.g., \"Is feature X <= threshold?\").\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
